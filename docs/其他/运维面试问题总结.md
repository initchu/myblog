# IT运维面试问题总结



## 一、集群相关基础组件



### （一）ETCD



#### 1. 概念



etcd 是 CoreOS 团队发起的开源项目，是一个管理配置信息和服务发现（service discovery）的项目，目标是构建高可用的分布式键值（key-value）数据库，基于 Go 语言实现。



#### 2. 特点



- **简单**：支持 REST 风格的 HTTP+JSON API。

- **安全**：支持 HTTPS 方式的访问。

- **快速**：支持并发 1k/s 的写操作。

- **可靠**：支持分布式结构，基于 Raft 一致性算法（通过选举主节点实现分布式系统一致性）。



#### 3. 适应场景



- **服务发现（Service Discovery）**：解决分布式集群中进程/服务的查找与连接问题，本质是通过名字查找监听 UDP/TCP 端口的进程并建立连接。

- **消息发布与订阅**：构建配置共享中心，数据提供者发布消息，使用者订阅主题，实现分布式系统配置的集中管理与动态更新（如将应用配置存于 etcd）。

- **负载均衡**：依托自身分布式架构，集群化后每个核心节点可处理用户请求，适合存储小数据量、高访问频率的消息以实现负载均衡。

- **分布式通知与协调**：基于 Watcher 机制，通过注册与异步通知，实现分布式系统间的数据变更实时处理。

- **分布式锁**：基于 Raft 算法保证数据强一致性，支持“保持独占”“控制时序”两种锁服务使用方式。

- **集群监控与 Leader 竞选**：实现简单且实时性强。



### （二）HAProxy



#### 1. 概念



HAProxy 是提供高可用性、负载均衡及基于 TCP 和 HTTP 应用代理的免费、快速、可靠解决方案，适用于并发量大（1w+）且需会话保持或七层处理的 Web 站点，可安全整合至现有架构并保护 Web 服务器不暴露到网络。



#### 2. 主要特性



- 可靠性和稳定性优异，可与硬件级 F5 负载均衡设备媲美。

- 支持高并发：最多维护 40000-50000 个并发连接，单位时间最大请求数 20000 个，最大处理能力达 10Git/s。

- 负载均衡能力：支持 8 种负载均衡算法，支持会话保持。

- 灵活扩展：支持虚拟主机功能，实现更灵活的 Web 负载均衡。

- 独特功能：支持连接拒绝、全透明代理。

- 访问控制：拥有强大的 ACL 支持。

- 高效数据结构：独特的弹性二叉树数据结构，查询速度复杂度为 O(1)，不受数据条目增加影响。

- 性能优化：支持客户端 keepalive、TCP 加速（零复制，类似 mmap 机制）、响应池（response buffering）。

- 多协议支持：支持 RDP 协议。

- 粘性策略：基于源的粘性（类似 Nginx 的 ip_hash），同一客户端请求在一定时间内调度到同一上游服务器。

- 监控与管理：提供更好的统计数据接口（显示后端服务器接收/发送/拒绝/错误数据）、详细健康状态检测（含管理功能）、基于流量的健康评估机制、HTTP 认证、命令行管理接口。

- 日志分析：自带日志分析器。



#### 3. 常见负载均衡策略



|策略名称|说明|
|---|---|
|roundrobin|简单轮询，依次分配请求到后端节点|
|static-rr|基于权重分配，权重高的节点优先获得请求，分配连接数更多|
|leastconn|最少连接者优先处理，将请求分配给当前连接数最少的节点|
|source|基于请求源 IP 分配，类似 Nginx 的 ip_hash 机制，保证同一 IP 请求到同一节点|
|ri|基于请求的 URI 分配|
|rl_param|基于 HTTP 请求头锁定每一次 HTTP 请求|
|rdp-cookie(name)|基于 cookie(name) 锁定并哈希每一次 TCP 请求|


### （三）Heartbeat



#### 1. 概念



Heartbeat 是 Linux-HA 项目的组件，提供心跳检测、资源接管、集群服务监测、失效切换等功能，核心功能为**心跳监测**和**资源接管**。



#### 2. 核心机制



- 心跳监测：通过网络链路或串口进行，支持冗余链路，节点间相互发送报文告知自身状态；若指定时间内未收到对方报文，判定对方失效。

- 资源接管：当检测到节点失效时，启动资源接管模块，接管失效节点上的资源或服务。



### （四）Keepalived



#### 1. 概念



Keepalived 是基于 VRRP 协议的 LVS 服务高可用方案，解决静态路由单点故障问题。



#### 2. 工作原理



- 集群角色：包含主服务器（MASTER）和备份服务器（BACKUP），对外表现为一个虚拟 IP（VIP）。

- 心跳机制：主服务器定期发送 VRRP 通告信息给备份服务器；若备份服务器收不到 VRRP 消息（主服务器异常），则接管虚拟 IP，继续提供服务，保证高可用。



#### 3. 主要模块及作用



|模块|作用|
|---|---|
|core|核心模块，负责主进程启动、维护，以及全局配置文件的加载和解析|
|vrrp|实现 VRRP 协议，负责虚拟 IP 的管理和主备节点切换|
|check|健康检查模块，支持端口检查、URL 检查等方式，监测后端节点或服务状态|


#### 4. 健康检查机制（保证高可用）



Keepalived 工作在 TCP/IP 模型的网络层（3 层）、传输层（4 层）和应用层（7 层），通过多维度健康检查剔除故障节点：



- **网络层**：采用 ICMP 协议向集群节点发送 ICMP 数据包，无响应则判定节点失效，从集群剔除。

- **传输层**：利用 TCP 端口连接和扫描技术，检测节点特定端口（如 Web 服务 80 端口、SSH 22 端口），无响应则剔除节点。

- **应用层**：支持 FTP、telnet、smtp、dns 等高层协议，用户可自定义监测逻辑，若监测结果与正常预期不符，剔除对应节点。



### （五）LVS



#### 1. 概念



LVS（Linux Virtual Server，Linux 虚拟服务器）是虚拟服务器集群系统，可在 Unix/Linux 平台实现负载均衡集群功能。



#### 2. 主要作用



- 负载分担：将大规模并发访问/数据流量分担到多台节点设备，减少用户响应时间，提升体验。

- 并行处理：将重负载运算分担到多台节点并行处理，汇总结果返回用户，提升系统处理能力。

- 高可用保障：实现 7*24 小时服务，单个/多个节点宕机不影响业务，集群负载均衡接收所有服务请求。



#### 3. 工作模式及工作过程



|工作模式|原理|优点|缺点|
|---|---|---|---|
|VS/NAT（NAT 模式）|1. 负载均衡器接收客户端请求，按调度算法选择后端真实服务器（RS）；<br>2. 改写请求包目标 IP/端口为 RS 的 IP（RIP）；<br>3. RS 响应后，按默认路由将包发给负载均衡器；<br>4. 负载均衡器改写响应包源 IP 为虚拟 IP（VIP），返回客户端。|集群服务器支持任意 TCP/IP 操作系统，负载均衡器只需一个合法 IP。|扩展性有限，所有请求/应答需经负载均衡器，易成为系统瓶颈。|
|VS/TUN（隧道模式）|1. 负载均衡器接收请求，选择 RS；<br>2. 封装请求包为 IP 隧道（T-IP），转发给 RS；<br>3. RS 响应后，直接发给客户端，无需经负载均衡器。|负载均衡器仅分发请求，RS 直接应答客户端，减少负载均衡器数据流动，无瓶颈，支持大请求量。|RS 需合法 IP，且所有服务器需支持“IP Tunneling”。|
|VS/DR（直接路由模式）|1. 负载均衡器接收请求，选择 RS；<br>2. 改写请求包目标 MAC 地址为 RS 的 MAC（R-MAC）；<br>3. RS 响应后，直接发给客户端，无需经负载均衡器。|同隧道模式，负载均衡器无数据瓶颈，支持大请求量。|负载均衡器与 RS 需在同一物理网段（同一局域网），网卡需连接同一网段。|


#### 4. 常见调度算法（均衡策略）



LVS 调度算法分**固定调度算法**和**动态调度算法**两类：



- **固定调度算法**：

    - rr（轮询）：依次分配请求到 RS，适合 RS 处理性能接近的场景。

    - wrr（加权轮询）：按 RS 权值分配，权值高的优先获请求，连接数更多；同权值 RS 连接数相同。

    - dh（目的地址哈希）：以目的地址为关键字查静态 hash 表，获取目标 RS。

    - sh（源地址哈希）：以源地址为关键字查静态 hash 表，获取目标 RS。

- **动态调度算法**：

    - wlc（加权最小连接数）：计算各 RS 的 Ti/Wi（Ti 为当前 TCP 连接数，Wi 为权值），选择最小值对应的 RS。

    - lc（最小连接数）：IPVS 表存储活动连接，将请求发给当前连接最少的 RS。

    - lblc（基于地址的最小连接数）：同一目的地址请求分配给未满负荷的同一 RS；否则分配给连接数最少的 RS，作为下次优先选择。

    - lblcr（基于地址的带复制的最小连接数）：在 lblc 基础上，支持复制机制，提升负载均衡效果。



## 二、负载均衡核心对比



### （一）四层与七层负载均衡的区别



|维度|四层负载均衡（4 层交换机）|七层负载均衡（7 层交换机）|
|---|---|---|
|工作层级|TCP/IP 模型的 IP 层（3 层）+ TCP/UDP 层（4 层）|OSI 模型的应用层（7 层）|
|实现原理|基于 IP+端口分析流量，实现负载均衡|基于报文内容（如 HTTP 头、URI），结合负载均衡算法选择后端服务器（“内容交换器”）|
|支持协议|TCP、UDP 等传输层协议|HTTP、FTP、SMTP 等应用层协议|
|典型产品|LVS、F5|HAProxy、Nginx|


### （二）LVS、Nginx、HAProxy 的异同



#### 1. 相同点



三者均为**软件负载均衡产品**，用于实现请求分发和服务高可用。



#### 2. 不同点



|对比维度|LVS|HAProxy|Nginx|
|---|---|---|---|
|实现基础|基于 Linux 操作系统实现（内核层）|基于第三方应用实现（应用层）|基于第三方应用实现（应用层）|
|支持层级|仅 4 层（IP 负载均衡），无法基于目录/URL 转发|支持 4 层（TCP）和 7 层（HTTP），提供综合负载均衡解决方案|支持 4 层和 7 层|
|状态监测功能|单一，仅基础节点状态检测|丰富强大，支持端口、URL、脚本等多种检测方式|支持端口检测，不支持 URL 检测|
|性能|最优，工作在 4 层，无流量产生，资源消耗低，抗负载能力强|优于 Nginx（并发处理），但低于 LVS（4 层模式）|较高，支持几万并发，负载度低于 LVS|
|功能扩展性|不支持正则、动静分离|支持 Session 保持、Cookie 引导、TCP 协议转发|支持正则（比 HAProxy 灵活）、静态资源服务、反向缓存、Web 服务器功能|
|适用场景|大型集群、高并发场景（如数据库负载均衡）|中大型集群、需 7 层高级特性（如 URL 检测、Session 保持）的场景|Web 服务（静态资源、反向代理）、中小型集群负载均衡|


## 三、代理服务器



### 1. 概念



代理服务器是位于客户端与原始（资源）服务器之间的服务器，客户端通过向代理服务器发送请求（指定目标原始服务器），由代理服务器转交请求并将获取的内容返回给客户端。



### 2. 主要作用



- **资源获取**：代替客户端从原始服务器获取资源。

- **加速访问**：代理服务器可能离原始服务器更近，减少网络延迟。

- **缓存作用**：保存从原始服务器获取的资源，客户端可快速获取缓存内容。

- **隐藏真实地址**：代理服务器代替客户端请求，隐藏客户端真实 IP 等信息。



## 四、高可用与分布式理论



### （一）高可用集群的衡量维度（RTO & RPO）



|维度|概念|最优情况|最坏情况|
|---|---|---|---|
|RTO（Recovery Time Objective，恢复时间目标）|服务从故障到恢复的时间，衡量服务恢复速度|0（立即恢复）|无穷大（永久无法恢复）|
|RPO（Recovery Point Objective，恢复点目标）|灾难发生时允许丢失的数据量，衡量数据一致性|0（无数据丢失，同步数据）|大于 0（如 RPO=1d 表示丢失 1 天数据）|


**核心**：最佳恢复状态为 RTO=RPO=0，几乎无法实现，需根据业务需求平衡。



### （二）CAP 理论



#### 1. 核心定义



CAP 理论指出分布式系统需满足三个条件，最多同时较好满足两个：



- **Consistency（一致性）**：所有节点在同一时间具有相同的数据。

- **Availability（可用性）**：保证每个请求无论成功/失败都有响应。

- **Partition tolerance（分区容错性）**：系统中任意信息丢失或节点失败不影响系统继续运行。



#### 2. 核心结论



分布式系统无法同时满足一致性、可用性、分区容错性，需根据场景取舍（如分布式数据库优先一致性，Web 服务优先可用性）。



### （三）ACID 理论



ACID 是数据库事务的四个特性，保证事务的可靠性：



- **原子性（Atomicity）**：事务是不可分割的整体，要么全部执行，要么全部不执行（无部分执行）。

- **一致性（Consistency）**：事务执行前、后数据库状态均一致（如转账前后总金额不变）。

- **隔离性（Isolation）**：事务未提交前，其操作的数据对其他用户不可见，避免并发干扰。

- **持久性（Durable）**：事务成功提交后，变更永久保存（记录到 redo 日志），即使系统故障也不丢失。



## 五、Kubernetes（K8s）



### （一）基础概念



|概念|定义|
|---|---|
|Master|K8s 集群管理节点，负责集群管理和资源数据访问入口；运行 Etcd（可选）、kube-apiserver、kube-controller-manager、kube-scheduler|
|Node（Worker）|运行 Pod 的服务节点，Pod 运行的宿主机；运行 Docker Engine、kubelet、kube-proxy|
|Pod|运行于 Node 上，若干相关容器的组合；容器共享网络命名空间、IP、端口（可通过 [localhost](http://localhost) 通信）；K8s 最小调度单位，可包含 1 个或多个容器|
|Label|键值对（Key/Value），可附加到 Node、Pod、Service 等资源；通过 Label Selector 筛选资源|
|Replication Controller（RC）|管理 Pod 副本，保证集群中 Pod 副本数量与指定数量一致；核心用于弹性伸缩、动态扩容、滚动升级|
|Deployment|基于 RC 升级，支持查看 Pod 部署进度；内部使用 RS（Replica Set）实现|
|HPA（Horizontal Pod Autoscaler）|Pod 横向自动扩容资源，通过分析 RC 控制的 Pod 负载，调整副本数量|
|Service|定义 Pod 逻辑集合和访问策略，抽象真实服务；提供统一访问入口、代理和发现机制，关联相同 Label 的 Pod|
|Volume|Pod 中多容器可访问的共享目录，定义在 Pod 上，可被 1 个或多个容器挂载|
|Namespace|实现多租户资源隔离，将资源分配到不同 Namespace（如项目、小组），便于共享集群资源的同时分别管理|


### （二）核心组件及作用



#### 1. Master 控制组件



|组件名称|作用|
|---|---|
|kube-apiserver|K8s 系统入口，封装核心对象增删改查操作（RESTful API），集群内组件通信枢纽|
|kube-scheduler|为新创建的 Pod 选择节点（调度），负责集群资源调度|
|kube-controller-manager|执行各种控制器，保证集群正常运行，包含多个子控制器|
|Replication Controller|管理 RC，关联 RC 与 Pod，保证副本数量一致|
|Node Controller|管理 Node，定期检查健康状态，标识失效/有效节点|
|Namespace Controller|管理 Namespace，清理无效 Namespace 及旗下资源（Pod、Service 等）|
|Service Controller|管理 Service，提供负载均衡和服务代理|
|EndPoints Controller|管理 Endpoints，关联 Service 与 Pod，实时更新 Endpoints（Pod 变化时）|
|Service Account Controller|管理 Service Account，为每个 Namespace 创建默认 Service Account 及对应的 Secret|
|Persistent Volume Controller|管理 PV（Persistent Volume）和 PVC（Persistent Volume Claim），实现 PV 与 PVC 绑定、清理回收|
|Daemon Set Controller|管理 DaemonSet，创建 Daemon Pod，保证指定 Node 运行 Daemon Pod|
|Deployment Controller|管理 Deployment，关联 Deployment 与 RC，保证 Pod 数量，实现更新|
|Job Controller|管理 Job，为 Job 创建一次性任务 Pod，保证完成指定任务数|
|Pod Autoscaler Controller|实现 Pod 自动伸缩，定时获取监控数据，匹配策略后执行伸缩|


#### 2. Node 组件



- **kubelet**：运行在每个 Node 上的代理服务，处理 Master 下发的任务，管理 Pod 及容器；注册节点信息，定期汇报节点资源使用情况，通过 cAdvisor 监控容器和节点资源。

- **kube-proxy**：运行在所有 Node 上，监听 apiserver 中 Service 和 Endpoint 的变化，创建路由规则，提供 Service IP 和负载均衡功能（透明代理+负载均衡）。

- **Docker Engine**：容器运行时，负责容器的创建、启动、停止等生命周期管理。



### （三）关键工具与部署方式



#### 1. 核心工具



|工具名称|作用|
|---|---|
|Minikube|本地运行单节点 K8s 集群的工具，用于开发和测试|
|Kubectl|命令行工具，用于控制 K8s 集群（如检查资源、创建/删除/更新组件、查看应用）|
|cAdvisor|集成于 kubelet 的监控组件，实时采集节点及容器的性能指标（CPU、内存等）|


#### 2. 常见部署方式



- **kubeadm**：官方推荐方式，简化集群部署流程，支持快速初始化 Master 和 Node。

- **二进制部署**：手动下载并部署各组件（kube-apiserver、kube-controller-manager 等），灵活性高，适合生产环境，但配置复杂。

- **Minikube**：本地单节点部署，适合开发测试。



### （四）Pod 核心机制



#### 1. Pod 状态



|状态名称|说明|
|---|---|
|Pending|API Server 已创建 Pod，但 Pod 内至少一个容器镜像未创建（含下载中）|
|Running|Pod 内所有容器已创建，至少一个容器处于运行/启动/重启状态|
|Succeeded|Pod 内所有容器成功执行退出，且不会重启|
|Failed|Pod 内所有容器已退出，至少一个容器退出为失败状态（退出码非 0）|
|Unknown|无法获取 Pod 状态（如网络通信故障）|


#### 2. 创建 Pod 的主要流程



1. 客户端提交 Pod 配置（如 YAML 文件）到 kube-apiserver。

2. apiserver 通知 controller-manager 创建资源对象。

3. controller-manager 通过 apiserver 将 Pod 配置存储到 Etcd。

4. kube-scheduler 检测到 Pod 信息，执行**预选**（过滤不符合资源要求的 Node）和**优选**（挑选最优 Node），将配置发送到目标 Node 的 kubelet。

5. kubelet 根据配置运行 Pod，运行成功后将状态返回给 scheduler，scheduler 存储状态到 Etcd。



#### 3. 重启策略（RestartPolicy）



Pod 重启策略应用于所有容器，由 kubelet 在 Node 上执行，默认值为 **Always**：



|策略名称|说明|
|---|---|
|Always|容器失效时，kubelet 自动重启容器|
|OnFailure|容器终止且退出码非 0 时，kubelet 自动重启容器|
|Never|无论容器状态如何，kubelet 不重启容器|


**策略与控制器的关联限制**：



- RC 和 DaemonSet：必须设为 Always（保证容器持续运行）。

- Job：设为 OnFailure 或 Never（确保执行完成后不重启）。

- kubelet（静态 Pod）：Pod 失效时重启，不进行健康检查（无论策略如何）。



#### 4. 健康检查方式（探针）



K8s 通过三类探针检查 Pod 健康状态：



|探针类型|作用|应用场景|
|---|---|---|
|LivenessProbe|判断容器是否存活（running 状态）；探测失败则 kubelet 杀掉容器，按重启策略处理；无探针则默认“Success”|保证容器运行状态正常（如应用死锁时重启）|
|ReadinessProbe|判断容器是否启动完成（ready 状态）；探测失败则修改 Pod 状态，Endpoint Controller 从 Service Endpoint 中移除该 Pod|避免请求发送到未就绪的容器（如应用初始化中）|
|StartupProbe|启动检查，针对启动缓慢的业务；避免业务未启动完成被 LivenessProbe/ReadinessProbe 误杀|如数据库、大型中间件（启动耗时较长）|


#### 5. LivenessProbe 探针的常见方式



- **ExecAction**：在容器内执行命令，返回码为 0 则健康。

- **TCPSocketAction**：通过容器 IP 和端口建立 TCP 连接，连接成功则健康。

- **HTTPGetAction**：通过容器 IP、端口和路径调用 HTTP GET 方法，响应状态码 200-399 则健康。



#### 6. 常见调度方式



|调度方式|说明|
|---|---|
|Deployment/RC|自动部署多份 Pod 副本，监控副本数量，维持指定数量（弹性伸缩、滚动升级）|
|NodeSelector|定向调度：通过 Node 标签（Label）与 Pod 的 nodeSelector 匹配，将 Pod 调度到指定 Node|
|NodeAffinity（亲和性调度）|扩展调度能力，分两类规则：<br>1. requiredDuringSchedulingIgnoredDuringExecution（硬规则）：必须满足才调度；<br>2. preferredDuringSchedulingIgnoredDuringExecution（软规则）：优先满足，不强求，可设权重|
|Taints 和 Tolerations（污点和容忍）|Taints（污点）：使 Node 拒绝特定 Pod 运行；<br>Tolerations（容忍）：Pod 属性，允许运行在有 Taints 的 Node 上|
|静态 Pod|由 kubelet 管理，仅存在于特定 Node，无法通过 API Server 管理，不关联控制器，无健康检查|


### （五）Deployment 与 DaemonSet



#### 1. Deployment 升级过程



1. 初始创建 Deployment 时，系统创建一个 ReplicaSet（RS），并创建指定数量的 Pod 副本。

2. 更新 Deployment 时，系统创建新 RS，将其副本数扩展到 1，同时将旧 RS 缩减到（原数量-1）。

3. 按更新策略逐个调整新旧 RS 的副本数（如每次新增 1 个新 Pod，删除 1 个旧 Pod）。

4. 最终新 RS 运行指定数量的新版本 Pod，旧 RS 副本数缩减为 0。



#### 2. Deployment 升级策略



通过 `spec.strategy` 指定，默认值为 **RollingUpdate**：



|策略名称|说明|
|---|---|
|Recreate（重建）|更新时先杀掉所有运行中的旧 Pod，再创建新 Pod；适合无状态服务，会有服务中断|
|RollingUpdate（滚动更新）|逐个更新 Pod，保证服务不中断；可通过 `maxUnavailable`（最大不可用 Pod 数）和 `maxSurge`（最大超出 Pod 数）控制更新节奏|


#### 3. DaemonSet 特性



- **部署规则**：在每个 K8s 集群 Node 上运行一个 Pod，每个 Node 仅运行一个，不支持定义 `replicas`（副本数由 Node 数量决定）。

- **典型场景**：

    - 日志收集（如 Fluentd、Logstash）。

    - 节点监控（如 Prometheus Node Exporter）。



### （六）Service 与网络



#### 1. Service 类型



|类型|作用|访问方式|
|---|---|---|
|ClusterIP|虚拟服务 IP，仅集群内部 Pod 可访问；kube-proxy 通过 iptables 规则转发|集群内通过 ClusterIP:Port 访问|
|NodePort|将 Service 端口映射到宿主机端口；外部客户端通过 Node IP:NodePort 访问|外部：Node IP:NodePort；内部：ClusterIP:Port 或 Node IP:NodePort|
|LoadBalancer|结合外接负载均衡器（如公有云 LB），将外部流量分发到 Service；需指定 `spec.status.loadBalancer` 中的 LB IP|外部通过 LB IP:Port 访问，LB 自动转发到后端 Pod|


#### 2. Service 分发策略



- **RoundRobin（轮询）**：默认策略，依次转发请求到后端 Pod。

- **SessionAffinity（会话保持）**：基于客户端 IP，第一次转发到某个 Pod 后，后续同一客户端请求均转发到该 Pod。



#### 3. Headless Service



- **作用**：不提供默认负载均衡，不为 Service 设置 ClusterIP；仅通过 Label Selector 返回后端 Pod 列表，由客户端自行处理负载均衡。

- **适用场景**：需手动指定负载均衡器、应用需知道同组服务其他实例（如分布式数据库集群）。



#### 4. 外部访问集群内服务的方式



- **Pod 端口映射（hostPort）**：将 Pod 端口映射到宿主机，外部通过“宿主机 IP:hostPort”访问。

- **Service NodePort**：通过 NodePort 类型 Service，外部通过“Node IP:NodePort”访问。

- **Service LoadBalancer**：公有云场景，通过外接 LB 访问（LB 自动关联 NodePort）。

- **Ingress**：通过 Ingress 资源和 Ingress Controller，实现 HTTP 层路由（如基于域名、URL 路径转发到不同 Service）。



#### 5. Ingress



- **概念**：Ingress 是 K8s 资源对象，用于将不同 URL 请求转发到后端不同 Service，实现 HTTP 层业务路由；需结合 Ingress Controller（如 Nginx Ingress Controller）实现负载均衡。

- **工作流程**：Ingress Controller 监听 Ingress 规则和 Service/Endpoint 变化，生成反向代理配置（如 Nginx 配置），直接将客户端请求转发到后端 Pod（跳过 kube-proxy），流程为“Ingress Controller + Ingress 规则 → Services → Pods”。



### （七）存储与持久化



#### 1. 数据持久化方式



|存储类型|定义|特性|适用场景|
|---|---|---|---|
|EmptyDir（空目录）|Pod 内的临时共享目录，由 Pod 内部映射到宿主机|同 Pod 内容器共享；Pod 删除时数据丢失；生命周期与 Pod 一致|临时数据存储（如合并/排序算法）、容器间数据共享|
|HostPath|将宿主机已存在的目录/文件挂载到容器内部|数据存储在宿主机，Pod 删除后数据不丢失；Pod 与 Node 耦合（仅能在指定 Node 恢复）|需持久化但不要求跨 Node 访问的场景（如日志存储到宿主机）|
|PersistentVolume（PV）|对底层网络共享存储（如 NFS、GFS）的抽象，定义为集群“资源”|独立于 Pod 生命周期；支持动态/静态供应；可跨 Node 访问|有状态服务（如数据库）的数据持久化|


#### 2. PV 与 PVC



- **PV（Persistent Volume）**：集群级存储资源，由管理员创建，描述底层存储特性（如容量、访问模式）。

- **PVC（Persistent Volume Claim）**：用户对存储资源的“申请”，指定所需存储容量、访问模式等，系统自动匹配 PV 并绑定。



#### 3. PV 生命周期阶段



|阶段名称|说明|
|---|---|
|Available|可用状态，未与任何 PVC 绑定|
|Bound|已与某个 PVC 绑定|
|Released|绑定的 PVC 已删除，资源已释放，但未被集群回收|
|Failed|自动资源回收失败|


#### 4. 存储供应模式



- **静态模式（Static）**：管理员手工创建多个 PV，定义底层存储特性；用户创建 PVC 后，系统匹配 PV 绑定。

- **动态模式（Dynamic）**：管理员通过 StorageClass 描述后端存储类型；用户创建 PVC 时指定 StorageClass，系统自动创建 PV 并绑定。



#### 5. CSI 模型（Container Storage Interface）



- **概念**：K8s 容器存储接口标准，存储提供方基于标准接口实现插件，解耦存储插件与 K8s 核心代码，部署独立。

- **核心组件**：

    - **CSI Controller**：从存储服务视角管理存储资源和存储卷（如创建/删除 PV）。

    - **CSI Node**：管理 Node 上的 Volume（如挂载/卸载 Volume 到容器）。



### （八）自动扩容与监控



#### 1. HPA（Horizontal Pod Autoscaler）自动扩容机制



- **原理**：

    1. Metrics Server（如 Heapster）持续采集所有 Pod 副本的指标（CPU、内存）。

    2. HPA 控制器通过 Metrics Server API 获取指标，基于用户定义的扩缩容规则（如 CPU 使用率阈值）计算目标 Pod 副本数。

    3. 若目标副本数与当前不同，HPA 向 Deployment/RC/ReplicaSet 发起 scale 操作，调整 Pod 数量。

- **支持指标**：核心指标（CPU、内存）、自定义指标（如请求 QPS，需 Prometheus 等组件）。



#### 2. Metric Service



- **核心指标（Core Metrics）**：Node、Pod 的 CPU 和内存使用指标，K8s 1.10+ 默认通过 Metrics Server 采集。

- **自定义指标（Custom Metrics）**：如请求量、响应时间等，由 Prometheus、Grafana 等组件实现。



#### 3. EFK 日志统一管理



- **组件构成**：

    - **Elasticsearch**：搜索引擎，负责存储日志和提供查询接口。

    - **Fluentd**：日志收集器，以 DaemonSet 方式部署在每个 Node，监控并收集 Node 上的系统日志和容器日志（挂载 `/var/lib/docker/containers` 和 `/var/log` 目录），处理后发送到 Elasticsearch。

    - **Kibana**：Web 可视化界面，用于浏览和搜索 Elasticsearch 中的日志。

- **工作流程**：Fluentd 收集日志 → 发送到 Elasticsearch 存储 → Kibana 展示日志。



### （九）安全机制



#### 1. 安全控制维度



- **基础设施隔离**：保证容器与宿主机的隔离（如命名空间、资源限制）。

- **最小权限原则**：限制组件权限，确保组件仅执行授权操作。

- **用户权限划分**：区分普通用户和管理员角色，避免越权操作。

- **API Server 认证授权**：

    - **认证（Authentication）**：通过 HTTPS、Token 识别客户端身份。

    - **授权（Authorization）**：通过 RBAC 等策略判断 API 调用是否合法。

- **敏感数据保护**：通过 Secret 存储密码、Token 等敏感数据（而非明文存储）。

- **准入机制（AdmissionControl）**：API 请求经过认证授权后，执行准入操作（如资源限制检查、ServiceAccount 自动配置），最后操作目标对象。



#### 2. 准入控制（AdmissionControl）



- **执行顺序**：认证 → 授权 → 准入控制 → 操作对象。

- **常用准入组件**：

    - **AlwaysAdmit**：允许所有请求。

    - **AlwaysDeny**：禁止所有请求（测试环境用）。

    - **ServiceAccount**：自动配置 ServiceAccount（如 Pod 无 ServiceAccount 时添加 default）。

    - **LimitRanger**：检查请求是否违反 Namespace 中的 LimitRange 约束（如资源限制）。

    - **NamespaceExists**：拒绝创建不存在的 Namespace 的请求。



#### 3. RBAC（基于角色的访问控制）



- **概念**：通过角色（Role）定义权限，将角色绑定到用户/用户组（RoleBinding），实现权限管理。

- **优势**：

    - 覆盖集群内所有资源和非资源权限。

    - 基于 API 对象实现，可通过 kubectl/API 操作。

    - 运行时可调整，无需重启 API Server。



#### 4. Secret



- **作用**：保管私密数据（密码、OAuth Tokens、SSH Keys 等），比明文存储更安全，便于分发。

- **使用方式**：

    1. Pod 通过 ServiceAccount 自动使用 Secret。

    2. 将 Secret 挂载为 Pod 内的文件或环境变量。

    3. 镜像下载时，通过 Pod 的 `spec.ImagePullSecrets` 引用 Secret（用于私有镜像仓库认证）。



#### 5. PodSecurityPolicy



- **概念**：精细控制 Pod 对资源的使用方式和安全策略；开启后，默认禁止创建 Pod，需创建 PodSecurityPolicy 和 RBAC 授权策略才能创建 Pod。

- **支持的安全策略**：

    - 特权模式控制（`privileged`：是否允许 Pod 以特权模式运行）。

    - 宿主机资源控制（如 `hostPID`：是否共享宿主机进程空间）。

    - 用户/组控制（设置容器运行的 UID/GID 范围）。

    - 权限提升控制（`AllowPrivilegeEscalation`：容器子进程是否可提升权限）。

    - SELinux 配置。



### （十）网络模型与组件



#### 1. K8s 网络模型



- **核心原则**：每个 Pod 拥有独立 IP，所有 Pod 处于扁平网络空间，可直接通过 IP 通信（无论是否在同一 Node）。

- **Pod 内网络**：同一 Pod 内的容器共享网络命名空间（IP、端口、网络设备），可通过 [localhost](http://localhost) 通信。

- **IP 分配**：IP 以 Pod 为单位分配，Pod 内所有容器共享该 IP。



#### 2. CNI 模型（Container Network Interface）



- **概念**：容器网络插件化标准，定义容器网络操作规范，通过插件实现网络配置。

- **核心概念**：

    - **容器（Container）**：拥有独立 Linux 网络命名空间的环境（如 Docker 容器）。

    - **网络（Network）**：可互连的实体集合（容器、物理机、路由器等），各实体有唯一 IP。

- **插件类型**：

    - **CNI Plugin**：负责为容器配置网络资源（如分配 IP、创建网络设备）。

    - **IPAM Plugin**：负责 IP 地址分配与管理（CNI Plugin 的一部分）。



#### 3. 网络策略（Network Policy）



- **作用**：实现 Pod 间网络访问的细粒度隔离，定义允许/禁止访问的客户端 Pod 列表。

- **工作原理**：Policy Controller 监听 Network Policy 定义，通过 Node 上的 Agent（结合 CNI 插件）设置网络访问规则。



#### 4. Flannel



- **作用**：K8s 底层网络实现方案，解决两个核心问题：

    1. 为每个 Node 上的 Docker 容器分配不冲突的 IP。

    2. 建立覆盖网络（Overlay Network），将数据包原封不动传递到目标容器。



#### 5. Calico



- **实现原理**：基于 BGP 协议的纯三层网络方案，与 K8s、OpenStack 等集成良好：

    1. 每个计算节点通过 Linux Kernel 实现 vRouter，负责数据转发。

    2. vRouter 通过 BGP 协议向整个 Calico 网络广播本节点容器的路由信息，自动设置跨节点路由规则。

    3. 容器间数据通过 IP 路由直接通信，无需 NAT、隧道或 Overlay Network，效率高。



### （十一）其他核心操作



#### 1. Worker 节点加入集群的过程



1. 在新 Worker 节点安装 Docker、kubelet、kube-proxy。

2. 配置 kubelet 和 kube-proxy 的启动参数，指定 Master URL（集群 Master 地址），启动服务。

3. kubelet 通过自动注册机制，将节点信息注册到 Master 的 API Server。

4. Master 接受注册后，将新节点纳入集群调度范围。



#### 2. 优雅的节点关机维护



1. 使用 `kubectl drain <node-name>` 驱逐节点上的所有 Pod（确保 Pod 调度到其他节点，服务不中断）。

2. 确认 Pod 全部驱逐后，执行关机维护。

3. 维护完成后，使用 `kubectl uncordon <node-name>` 解除节点封锁，允许 Pod 重新调度到该节点。



#### 3. 集群联邦



- **概念**：将多个 K8s 集群作为一个整体管理，支持跨数据中心/云平台的集群统一控制。

- **作用**：实现多集群资源调度、服务发现、配置共享，提升系统可用性和扩展性。



### （十二）Helm



#### 1. 概念



Helm 是 K8s 的软件包管理工具，类似 Ubuntu 的 apt、CentOS 的 yum，用于打包、分发、安装和管理 K8s 应用资源。



#### 2. 核心术语



- **Chart**：Helm 包，包含应用的所有 K8s 资源配置（YAML 文件），通常压缩为 `name-version.tgz` 格式。



#### 3. 优势



- **统一管理**：集中管理应用的分散 K8s 资源（Pod、Service、Deployment 等），支持配置更新。

- **模板复用**：分发和复用应用模板（Chart），简化部署流程。

- **版本控制**：管理应用版本，支持升级、回滚。

- **用户友好**：

    - 发布者：通过 Helm 打包应用、管理依赖、发布到仓库。

    - 使用者：无需编写复杂 YAML，通过 Helm 快速查找、安装、卸载应用。



### （十三）K8s 优缺点



#### 1. 优势



- **容器编排**：自动化部署、调度、伸缩容器。

- **轻量级**：资源消耗低，支持高密度部署。

- **开源**：社区活跃，生态丰富。

- **弹性伸缩**：基于 HPA 实现 Pod 自动扩缩容。

- **负载均衡**：通过 Service 实现请求分发。

- **可移植**：支持公有云、私有云、混合云、多云。

- **可扩展**：模块化、插件化，支持自定义扩展。

- **自动化**：自动部署、重启、复制、伸缩。



#### 2. 缺点



- **安装配置复杂**：尤其是生产环境的高可用部署。

- **管理繁琐**：组件多，需维护 Master、Node、网络、存储等。

- **运行编译耗时**：大规模集群部署和编译需较长时间。

- **成本较高**：需更多资源（如 Master 节点冗余），比简单部署昂贵。

- **过度设计**：简单应用无需 K8s，反而增加复杂度。



## 六、OpenShift



### 1. 概念



OpenShift 是基于 Red Hat Enterprise Linux、Docker 和 K8s 的容器应用平台，为企业级应用提供安全、可伸缩的多租户操作系统，集成应用运行时和库，最小化配置和管理开销。



### 2. 主要特性



- **自助服务平台**：开发人员通过 Source-to-Image（S2I）从模板或代码仓库创建应用；管理员可定义资源配额。

- **多语言支持**：支持 Java、Node.js、PHP、Perl、Ruby 及中间件（Apache httpd、Tomcat、JBoss EAP 等）。

- **自动化**：支持应用生命周期管理（自动重建、重新部署、伸缩、故障转移）。

- **用户界面**：提供 Web UI（部署监控）和 CLI（远程管理）。

- **协作**：支持组织内/社区项目共享。

- **可伸缩性与高可用**：容器多租户、分布式平台，支持弹性、高可用（自动发现故障、重新部署）。

- **容器可移植性**：基于标准容器镜像和 K8s，可部署到其他兼容平台。

- **开源**：无厂商锁定。

- **安全性**：SELinux 多层安全、RBAC 访问控制、集成 LDAP/OAuth 认证。

- **动态存储管理**：基于 K8s PV/PVC 实现静态/动态存储。

- **部署灵活性**：支持裸机、虚拟化、多厂商 IaaS 云。

- **企业级支持**：Red Hat 支持，第三方容器/应用认证。

- **日志与监控**：集中日志聚合、实时 metrics 收集与分析。

- **DevOps 集成**：支持微服务架构，易与 CI/CD 工具集成。



### 3. Projects



- **概念**：对 K8s 资源进行分组，实现多租户隔离；可分配资源配额（限制 Pod、Volume、Service 数量）。

- **作用**：允许用户组独立组织和管理资源，用户需授权才能访问项目；用户创建项目后自动获得访问权限。



### 4. 高可用实现



OpenShift 高可用分两方面：



- **基础设施 HA（主机）**：默认支持 Master 节点本机 HA（基于 Keepalived 等），保证 Master 高可用。

- **应用 HA（Pods）**：K8s 自动调度 Pod 副本，Pod 失效时重建；节点失效时，调度 Pod 到其他节点；应用需自行维护状态（如 HTTP 会话复制、数据库复制）。



### 5. SDN 网络实现



- **默认 Docker 网络局限**：仅主机内容器可通信，跨主机容器无法通信。

- **SDN 原理**：通过软件定义网络（SDN）解耦控制平面（流量处理软件）和数据平面（路由底层机制），实现跨集群容器通信；基于 Open vSwitch 创建 Overlay 网络。

- **SDN 插件**：

    1. **ovs-subnet**：默认插件，flat 网络，所有 Pod/Service 可通信。

    2. **ovs-multitenant**：多租户隔离，每个 Project 分配唯一 VNID（虚拟网络 ID），不同 Project 的 Pod/Service 不可通信。

    3. **ovs-network policy**：支持管理员通过 NetworkPolicy 定义自定义隔离策略。

- **集群网络限制**：Master 节点无法通过集群网络访问容器（除非 Master 同时为 Node 节点）。



### 6. 角色与身份验证



#### 1. 角色



- **集群级角色**：控制整个集群资源的访问权限。

- **本地角色**：控制单个 Project 内资源的访问权限。

- **关联方式**：用户/用户组可关联多个角色，获取对应权限。



#### 2. 支持的身份验证类型



- **Basic Authentication (Remote)**：用户输入用户名/密码，OpenShift 向远程标识提供者验证。

- **Request Header Authentication**：基于请求头（如 X-RemoteUser）验证，通常与认证代理配合。

- **Keystone Authentication**：集成 OpenStack Keystone v3，用户使用 Keystone 凭证登录。

- **LDAP Authentication**：用户使用 LDAP 凭证登录，OpenStack 搜索 LDAP 目录匹配条目并绑定验证。

- **GitHub Authentication**：基于 OAuth 集成 GitHub，用户使用 GitHub 凭证登录，可限制访问特定 GitHub 组织。



## 七、中间件



### 1. 概念



中间件是独立的系统软件或服务程序，位于客户机/服务器操作系统之间，连接两个独立应用程序或系统，实现不同技术间的资源共享和信息交换。



### 2. 核心作用



- **解耦**：降低应用程序间的耦合度，使应用专注于核心业务逻辑。

- **桥接**：实现不同技术栈、协议、数据格式的应用间通信（如 Java 应用与 Python 应用通信）。

- **复用**：提供通用功能（如日志、监控、事务管理），避免重复开发。
> （注：文档部分内容可能由 AI 生成）